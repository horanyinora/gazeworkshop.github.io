---
layout: project
urltitle:  "Gaze Estimation and Prediction in the Wild"
title: "Gaze Estimation and Prediction in the Wild"
categories: iccv, workshop, computer vision, robotics, machine learning, natural language processing, gaze estimation
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Gaze Estimation and Prediction in the Wild</h1></center>
    <center><h2>ICCV 2019 Workshop, Seoul, Korea</h2></center>
    <!-- <center><span style="color:#e74c3c;font-weight:400;">Time and location TBA</span></center> 
    <center>TODO - DATE Sunday June 16 2019, 8:45am -- 5:40pm, <span style="color:#e74c3c;font-weight:400;"> TODO -location TBA</span></center>-->
  </div>
</div>

<hr>

<div class="row" id="intro">
  <div class="col-md-12">
    <!--img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">-->
    <p> Image credit: [TODO - references, image]</p> 
  </div>
</div>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
The 1st Workshop on Gaze Estimation and Prediction in the Wild (GAZE 2019) at ICCV 2019 is a first-of-its-kind workshop focused on designing and evaluating deep learning methods for the task of gaze estimation and prediction. We aim to encourage and highlight novel strategies with a focus on robustness and accuracy in natural in-the-wild settings. This is expected to be achieved via novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training among other directions. This half-day workshop consists of three invited talks as well as lightning talks from industry contributors. 
    </p>
  </div>
</div> <br>   

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite extended abstracts for work on tasks related to 3D scene generation or tasks leveraging generated 3D scenes.
      Workshop topics may include but are not limited to:
    </p>
    <ul>
      <li>Proposal of novel eye detection, gaze estimation, and gaze prediction pipelines using deep convolutional neural networks</li>
      <li>Incorporating geometric and anatomical constraints into neural networks in a differentiable manner</li>
      <li>Exploring attention mechanisms to improve the estimation or prediction of users’ points of regard</li>
      <li>Demonstration of robustness to conditions where current methods fail (illumination, appearance, low-resolution etc.)</li>
      <li>Robust estimation from different data modalities such as RGB, depth, near infra-red, head pose, and eye region landmarks</li>
      <li>Leveraging additional cues such as task context, temporal information, eye movement classification</li>
      <li>Designing new accurate metrics to account for rapid eye movements in the real world</li>
      <li>Semi-supervised / unsupervised / self-supervised learning, meta-learning, domain adaptation and other related machine learning methods for gaze estimation</li>
      <li>Methods for temporal gaze estimation and prediction including bayesian methods</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> We invite authors to submit papers to our workshop (8-page ICCV format), to be presented at a poster session on acceptance. All submissions will go through a double-blind review process. Accepted papers at ICCV/CVPR 2019 are also invited to be presented at our poster session for increased exposure of the works and to foster discussion in the community. All contributions must be submitted through CMT in the following link: TBD.
    </p>
  </div>
</div><br>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>July 29, 2019</td>
        </tr>
        <tr>
          <td>Reviews Released to Authors</td>
          <td>August 12, 2019</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>August 16, 2019</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>August 30, 2019</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>October 27, 2019 (Morning)</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<!--div class="row" >
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<p><br /></p>
<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome and Introduction</td>
          <td>8:45am - 9:00am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 1</td>
          <td>9:00am - 9:25am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 2</td>
          <td>9:25am - 9:50am</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>9:50am - 10:10am</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>10:10am - 11:10am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 3</td>
          <td>11:10am - 11:35am</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 4</td>
          <td>11:35am - 12:00pm</td>
        </tr>
        <tr>
          <td>Lunch Break</td>
          <td>12:00pm - 1:30pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 5 (Industry Talks)</td>
          <td>1:30pm - 2:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 6</td>
          <td>2:00pm - 2:25pm</td>
        </tr>
        <tr>
          <td>Oral 1</td>
          <td>2:25pm - 2:45pm</td>
        </tr>
        <tr>
          <td>Oral 2</td>
          <td>2:45pm - 3:05pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>3:05pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 7</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Invited Speaker Talk 8</td>
          <td>4:25pm - 4:50pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>4:50pm - 5:40pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>-->

<!--br>
<div class="row">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<p><br /></p>
<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>-->

<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div>

<!--<div class="row">
  <div class="col-md-12">
    <a href="http://vladlen.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/vladlen.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://vladlen.info/">Vladlen Koltun</a></b> is a Senior Principal Researcher and the director of the Intelligent Systems Lab at Intel. The lab is devoted to high-impact basic research on intelligent systems. Previously, he has been a Senior Research Scientist at Adobe Research and an Assistant Professor at Stanford where his theoretical research was recognized with the National Science Foundation (NSF) CAREER Award (2006) and the Sloan Research Fellowship (2007).
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="http://www.cs.utexas.edu/users/grauman/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/grauman.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a></b> is a Professor in the Department of Computer Science at the University of Texas at Austin and a Research Scientist in Facebook AI Research (FAIR).  Her research in computer vision and machine learning focuses on visual recognition and search.  Before joining UT-Austin in 2007, she received her Ph.D. at MIT.  She is an Alfred P. Sloan Research Fellow and Microsoft Research New Faculty Fellow, a recipient of NSF CAREER and ONR Young Investigator awards, the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the International Joint Conference on Artificial Intelligence (IJCAI), the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013, and the Helmholtz Prize in 2017. 
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="https://www.inf.ethz.ch/personal/marc.pollefeys/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/pollefeys.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a></b> is a full professor and head of the Institute for Visual Computing of the Dept. of Computer Science of ETH Zurich which he joined in 2007.  He leads the Computer Vision and Geometry lab.  Previously he was with the Dept. of Computer Science of the University of North Carolina at Chapel Hill where he started as an assistant professor in 2002 and became an associate professor in 2005.  Before he was a postdoctoral researcher at the Katholieke Universiteit Leuven in Belgium, where he also received his M.S. and Ph.D. degrees in 1994 and 1999, respectively. His main area of research is computer vision.  One of his main research goals is to develop flexible approaches to capture visual representations of real world objects, scenes and events. Dr. Pollefeys has received several prizes for his research, including a Marr prize, an NSF CAREER award, a Packard Fellowship and a ERC Starting Grant. He is the author or co-author of more than 280 peer-reviewed papers.
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="https://cs.brown.edu/people/epavlick/index.html"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/ellie.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://cs.brown.edu/people/epavlick/index.html">Ellie Pavlick</a></b> is an Assistant Professor of Computer Science at Brown University, and an academic partner with Google AI. She received her PhD in Computer Science from the University of Pennsylvania. She is interested in building better computational models of natural language semantics and pragmatics: how does language work, and how can we get computers to understand it the way humans do?
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.purdue.edu/homes/aliaga/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/aliaga.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.purdue.edu/homes/aliaga/">Daniel Aliaga</a></b> does research primarily in the area of 3D computer graphics but overlaps with computer vision and visualization while also having strong multi-disciplinary collaborations outside of computer science. His research activities are divided into three groups: a) his pioneering work in the multi-disciplinary area of inverse modeling and design; b) his first-of-its-kind work in codifying information into images and surfaces, and c) his compelling work in a visual computing framework including high-quality 3D acquisition methods. Dr. Aliaga’s inverse modeling and design is particularly focused at digital city planning applications that provide innovative “what-if” design tools enabling urban stake holders from cities worldwide to automatically integrate, process, analyze, and visualize the complex interdependencies between the urban form, function, and the natural environment.
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="https://www.cse.iitb.ac.in/~sidch/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/sid.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a></b> is a Senior Research Scientist at Adobe Research, and Assistant Professor (on leave) of Computer Science and Engineering at IIT Bombay. His research focuses on richer tools for designing three-dimensional objects, particularly by novice and casual users, and on related problems in 3D shape understanding, synthesis and reconstruction. He received his PhD from Stanford University, followed by a postdoc at Princeton and a year teaching at Cornell. Apart from basic research, he is also the original author of the commercial 3D modeling package Adobe Fuse.
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="http://graphics.stanford.edu/~adai/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/angela.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://graphics.stanford.edu/~adai/">Angela Dai</a></b> is a postdoctoral researcher at the Technical University of Munich.  She received her Ph.D. in Computer Science at Stanford University advised by Pat Hanrahan. Her research focuses on 3D reconstruction and understanding with commodity sensors. She received her Masters degree from Stanford University and her Bachelors degree from Princeton University. She is a recipient of a Stanford Graduate Fellowship.
    </p>
  </div>
</div><br>
<div class="row">
  <div class="col-md-12">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/jiajun.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is a fifth-year PhD student at MIT, advised by Bill Freeman and Josh Tenenbaum. He received his undergraduate degree from Tsinghua University, working with Zhuowen Tu. He has also spent time at research labs of Microsoft, Facebook, and Baidu. His research has been supported by fellowships from Facebook, Nvidia, Samsung, Baidu, and Adobe. He studies machine perception, reasoning, and its interaction with the physical world, drawing inspiration from human cognition.
    </p>
  </div>
</div><br>-->


<div class="row" id="organizers">
  <div class="col-md-12">
    <b>Industry Participants</b>
    <p>The workshop also features presentations by representatives of the following companies:</p>
  </div>
</div>
<div class="row">
  <div class="col-md-3">
    <a href="https://www.samsung.com/"><img src="/static/img/samsung.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://www.nvidia.com/"><img src="/static/img/nvidia.jpg" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://www.tobii.com/"><img src="/static/img/tobii.jpg" /></a>
  </div>
</div><br>


<div class="row" >
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://hyungjinchang.wordpress.com/">
      <img class="people-pic" src="{{ "/static/img/people/hj.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/spark/">
      <img class="people-pic" src="{{ "/static/img/people/sp.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/spark/">Seonwook Park</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/zhang/">
      <img class="people-pic" src="{{ "/static/img/people/xz.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/zhang/">Xucong Zhang</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "/static/img/people/oh.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "/static/img/people/al.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div>

<hr>



<br>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>


<!--{:.paper}
<span>[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span>{:.papertitle}  
<span>D. Ritchie, K. Wang, and Y.a. Lin</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1811.12463, 2018</span>{:.journal}  -->


{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}
